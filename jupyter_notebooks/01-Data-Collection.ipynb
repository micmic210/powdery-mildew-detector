{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "1. Import necessary packages and configure the working directory. \n",
        "2. Authenticate and retrieve the mildew dataset from Kaggle. \n",
        "3. Prepare the dataset by organizing it into train, validation, and test splits. \n",
        "4. Ensure data integrity by removing any non-image files.\n",
        "5. Perform data augmentation to increase the diversity of the training data.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- Kaggle JSON file: Used for authentication and dataset download. \n",
        "- Dataset sourse: The mildew dataset hosted on [Kaggle](https://www.kaggle.com/codeinstitute/cherry-leaves).\n",
        "- Local directories: Structure for storing and splitting data. \n",
        "\n",
        "## Outputs\n",
        "\n",
        "1. Raw Dataset: Downloaded and unzipped into the specified folder\n",
        "2. Cleaned Dataset: Non-image files removed for consistency. \n",
        "3. Structured Data: Split into training (70%), validation (10%), and testing (20%) sets, organized in respective directories. \n",
        "4. Augmented Data: Augmented training images generated and stored in the training directory.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "- These steps are critical to ensure the dataset is properly prepared for model training and evaluation. \n",
        "-  By structuring, cleaning, and augmenting the data, we minimize errors during training and improve model accuracy and generalization.\n",
        "\n",
        "## Vision Control: \n",
        "- This notebook and the associated datasets are version controlled using Git. Each version of the dataset and the preprocessing steps are tracked to ensure reproducibility and facilitate experimentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "os.chdir('/workspace/powdery-mildew-detector')\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the Kaggle configuration directory to the current working directory and set permissions for the Kaggle authentication JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the Kaggle Dataset and Download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/mildew_dataset\"   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file, and delete the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_file(my_data_dir):\n",
        "    \"\"\"\n",
        "    Remove files that are not images from the dataset directory. \n",
        "    \"\"\"\n",
        "    image_extension = ('.png', '.jpg', '.jpeg')\n",
        "    folders = os.listdir(my_data_dir)\n",
        "    for folder in folders:\n",
        "        files = os.listdir(my_data_dir + '/' + folder)\n",
        "        # print(files)\n",
        "        i = []\n",
        "        j = []\n",
        "        for given_file in files:\n",
        "            if not given_file.lower().endswith(image_extension):\n",
        "                file_location = my_data_dir + '/' + folder + '/' + given_file\n",
        "                os.remove(file_location)  \n",
        "                i.append(1)\n",
        "            else:\n",
        "                j.append(1)\n",
        "                pass\n",
        "        print(f\"Folder: {folder} - has image file\", len(j))\n",
        "        print(f\"Folder: {folder} - has non-image file\", len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "remove_non_image_file(my_data_dir='inputs/mildew_dataset/cherry-leaves')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "    \"\"\"\n",
        "    Split the dataset into training, validation, and test sets. \n",
        "    \"\"\"\n",
        "    # Check if the sum of the ratios is equal to 1.0\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # Get the class labels in the dataset directory \n",
        "    labels = os.listdir(my_data_dir)  \n",
        "    # Check if the 'test' folder already exists \n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        # Create 'train', 'validation', and 'test' folders with class subfolders\n",
        "        for folder in ['train', 'validation', 'test']:\n",
        "            for label in labels:\n",
        "                os.makedirs(name=my_data_dir + '/' + folder + '/' + label)\n",
        "        # Iterate through each class label\n",
        "        for label in labels:\n",
        "            # Get the list of files in the current class label directory\n",
        "            files = os.listdir(my_data_dir + '/' + label)\n",
        "            random.shuffle(files)\n",
        "            # Calculate the number of files for train, validation, and test sets\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            count = 1\n",
        "            for file_name in files:\n",
        "                if count <= train_set_files_qty:\n",
        "                    # Move the file to the 'train' set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/train/' + label + '/' + file_name)\n",
        "\n",
        "                elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                    # Move the file to the 'validation' set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/validation/' + label + '/' + file_name)\n",
        "                else:\n",
        "                    # Move the file to the 'test' set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/test/' + label + '/' + file_name)\n",
        "\n",
        "                count += 1\n",
        "            # Remove the original class directory after all files are moved\n",
        "            os.rmdir(my_data_dir + '/' + label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for Data Split Ratios:\n",
        "- The chosen data split ratios of 70/10/20 are common in machine learning. \n",
        "    - 70% for training provides sufficient data for the model to learn the patterns.\n",
        "    - 10% for validation allows for unbiased evaluation of the model's performance during training and helps with hyperrarameter tuning. \n",
        "    - 20% for testing ensures a robust final evaluation of the model's generalization ability on unseen data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"inputs/mildew_dataset/cherry-leaves\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ImageDataGenerator is used to augment the training data with random transformations.\n",
        "- This helps to increase the diversity of the training set and improve the model's robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_gen = ImageDataGenerator(\n",
        "    rotation_range=20,  # Randomly rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the width\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height\n",
        "    shear_range=0.2,  # Randomly apply shearing transformations\n",
        "    zoom_range=0.2,  # Randomly zoom in or out on images\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for Data Augmentation Techniques:\n",
        "- The chosen data augmentation techniques help to introduce variability in the training data, making the model more robust to different orientations, positions, and scales of the leaves in the images. This can help to improve the model's generalization ability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply data augmentation to the training set\n",
        "train_path = f\"inputs/mildew_dataset/cherry-leaves/train\"\n",
        "train_data_gen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(128, 128),  # Resize images to 128x128\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Binary classification (healthy or infected)\n",
        "    save_to_dir=train_path,  # Save augmented images to the training directory\n",
        "    save_prefix='augmented_',  # Prefix for augmented image filenames\n",
        "    save_format='png'  # Save images in PNG format\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Augmented Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_augmented_images(generator, n_images=5):\n",
        "    \"\"\"\n",
        "    Visualizes augmented images from an ImageDataGenerator.\n",
        "\n",
        "    Args:\n",
        "      generator: The ImageDataGenerator.\n",
        "      n_images: The number of images to visualize.\n",
        "    \"\"\"\n",
        "    for i in range(n_images):\n",
        "        image, label = next(generator)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Augmented Image {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "visualize_augmented_images(train_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Value Handling\n",
        "- This dataset does not contain any missing values. However, in general, for image datasets, missing values could manifest as missing pixels or corrupted image regions.\n",
        "- Strategies for handling missing values in image data include:\n",
        "    - Removing images with missing data if the percentage of missing pixels is significant.\n",
        "    - Imputing missing pixels using techniques like interpolation or inpainting.\n",
        "    - Using data augmentation techniques that introduce missingness to make the model more robust to missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and Next Steps\n",
        "\n",
        "### Conclusions\n",
        "The data collection and preprocessing steps were successfully completed:\n",
        "- The Cherry Leaves dataset was downloaded from Kaggle.\n",
        "- Non-image files were removed to ensure data integrity.\n",
        "- The data was split into training, validation, and test sets.\n",
        "- Data augmentation was applied to the training set to increase diversity and improve model generalization.\n",
        "\n",
        "### Next Steps\n",
        "The preprocessed and augmented data is now ready for the next steps:\n",
        "- Exploratory data analysis will be performed to gain insights into the data.\n",
        "- A suitable model architecture will be selected and trained using the prepared data.\n",
        "- The trained model will be evaluated on the test set to assess its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
